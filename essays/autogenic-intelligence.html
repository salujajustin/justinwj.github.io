<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Autogenic Intelligence</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="../style.css" />
</head>
<body><div class="back-link"><a href="../index.html">← Back to Justin Wayne Johnson</a></div>
<header id="title-block-header">
<h1 class="title">Autogenic Intelligence</h1>
</header>
<p><strong>Future intelligent systems will be grown instead of
engineered</strong>. The next paradigm of diverse intelligence will
likely involve entities that learn to compose meaning, value, and
culture as a consequence of their perceived environment.</p>
<p>The <strong>current approach</strong> of modern AI primarily consists
of <strong>feature engineering learning</strong>, in which we engineer a
symptom of intelligence, i.e. "intelligent agents exhibit 'x' so we
should put the 'x' trait into our model". This creates a myopic view
into constructing generally intelligent beings. We should instead
offload human discovered learning mechanisms in favor of systems that
learn to discover these concepts themselves.</p>
<p><strong>What about other bioinspired fields like RL and Evolutionary
algorithms?</strong> I like these fields but they miss the broader
point. Both describe a biological mechanism with fixed scope and
resolution. Reinforcement learning refers to the agent as an individual
and focuses on growth and learning within its lifetime.
Evolutionary/genetic algorithms refer to a population as a collective
and focuses on growth and learning across lifetimes. Both use selection
as a mechanism for specific optimization<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
But maybe this is the wrong conceptual model. Is evolution/RL equivalent
to that of an optimizer over a single target? In some sense, we want the
model to diverge when something interesting happens.</p>
<p>To their credit, there are glimpses of innovation and creativity
within these mechanisms, think move 37 in Alphago, yet they are
inefficient and hard to come by. It's not so surprising that it
identifies actions humans have never done before when millions of trials
are performed. Yet we observe that when humans have really deep
insights, they are not mining millions of examples but have some
fundamental representation about the world that allows serendipity to
emerge.</p>
<p><strong>What will this look like?</strong> I'll attempt to speculate
(will update often):</p>
<ul>
<li><p>I expect a <strong>multiagent</strong> society of mind similar to
other human and animal intelligence. I assert that intelligence can not
emerge under a single agent framework<a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>. Further, the whole and
the parts are interdependent both with competency in their own domain.
In such a system, the parts contribute to the structure or purpose of
the whole, and the whole, in turn, gives meaning or function to the
parts (a Kantian Whole).</p></li>
<li><p>If it emerges from silicon, I expect the learning dynamics to
look alien to the learning found in biological life. <strong>The
underlying substrate directs the abstractions built upon it</strong>. I
do not think the digital reconstructions of the brain will yield
expected results. It is likely we already have the compute in some form
required to get superintelligence but the challenge is to build
abstractions that can <strong>efficiently utilize the
hardware</strong><a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>.</p></li>
<li><p>The system should experience <strong>continuous</strong> and
<strong>indefinite self improvement</strong>. Intelligence is therefore
generated naturally and automatically (<strong>Autocurriculum</strong>)
arising from the complex social dynamics of the system provided that the
environment <strong>empowers</strong> the agents to improve and agents
are <strong>plastic</strong> enough to learn. Notions of equilibrium
(e.g. <strong>Homeorhesis</strong>) within its parts, are common and
diverse.</p></li>
<li><p>What is being optimized by the whole won't look like
optimization. How do we define what is good art or music? <strong>Some
concepts might remain ineffable</strong>. Nevertheless, we can still
strive for them without a proxy metric which is prone to
oversimplification. Objectives are then an emergent property of the
constraints of the system. Steering the system in a favorable way will
therefore be a <strong>function of constraints rather than a specific
target</strong>. Many times, the act of formalizing something collapses
its true meaning.</p></li>
</ul>
<p><strong>The goal</strong> is to develop theories and systems that
create the conditions for intelligence to emerge. In turn, we will learn
about the nature of intelligent behavior, reality, and ourselves. Not to
mention the unprecedented innovation and positivity that comes as a
result. I'll point to <a
href="https://www.darioamodei.com/essay/machines-of-loving-grace">Dario
Amodei — Machines of Loving Grace</a> for just a few. It is not
<em>just</em> about passing a test, benchmark, or winning a game. It's
about inventing, generating new knowledge, and amplifying creativity in
all its forms.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">

<ol>
<li id="fn1"><p>There was this experiment I saw with simulating
chemotaxis in bacteria. Individual bacteria pay an energy cost to signal
food, which is irrational for a single cell, but this altruistic
signaling boosts population survival. The persistence of the species as
a whole was only possible through non optimal individual actions.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This is more of a semantic point since the term one vs
many is dependent on context but I digress.<a href="#fnref2"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See "The Hardware Lottery" paper.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
